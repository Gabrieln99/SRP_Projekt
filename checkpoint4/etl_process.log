2025-05-11 15:28:43,270 [ERROR] ETL process failed: MySQL connector JAR not found at: D:ax_	reca_godina\skladistenje_rudarenje\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\spark_session.py", line 25, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: D:ax_	reca_godina\skladistenje_rudarenje\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
2025-05-11 15:32:25,450 [ERROR] ETL process failed: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$14(Executor.scala:1163)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$14$adapted(Executor.scala:1155)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:193)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)
	at org.apache.spark.executor.Executor.updateDependencies(Executor.scala:1155)
	at org.apache.spark.executor.Executor.<init>(Executor.scala:330)
	at org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)
	at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:599)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)
	at org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)
	at org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)
	... 25 more
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\spark_session.py", line 18, in get_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 203, in __init__
    self._do_init(
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 296, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 421, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1587, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$14(Executor.scala:1163)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$14$adapted(Executor.scala:1155)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:193)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)
	at org.apache.spark.executor.Executor.updateDependencies(Executor.scala:1155)
	at org.apache.spark.executor.Executor.<init>(Executor.scala:330)
	at org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)
	at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:599)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)
	at org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)
	at org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)
	... 25 more

2025-05-11 15:32:25,514 [INFO] Closing down clientserver connection
2025-05-11 15:48:00,409 [ERROR] ETL process failed: 'DataFrame' object has no attribute 'airline_sk'
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 98, in transform_fact_flight
    airline_dim_df.airline_sk
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3129, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'airline_sk'
2025-05-11 15:48:00,422 [INFO] Closing down clientserver connection
2025-05-11 16:24:49,252 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `name` cannot be resolved. Did you mean one of the following? [`id`, `carrier`, `airline_name`].;
'Project [trim(carrier#1, None) AS carrier#256, trim('name, None) AS name#257]
+- Relation [id#0,carrier#1,airline_name#2] JDBCRelation(airline) [numPartitions=1]
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 13, in run_transformations
    dim_airline_df = transform_airline_dim(raw_data["airline"])
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\dimensions\dim_airline.py", line 28, in transform_airline_dim
    .select(
     ^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `name` cannot be resolved. Did you mean one of the following? [`id`, `carrier`, `airline_name`].;
'Project [trim(carrier#1, None) AS carrier#256, trim('name, None) AS name#257]
+- Relation [id#0,carrier#1,airline_name#2] JDBCRelation(airline) [numPartitions=1]

2025-05-11 16:24:49,282 [INFO] Closing down clientserver connection
2025-05-11 16:29:43,602 [INFO]  Starting data extraction
2025-05-11 16:29:48,603 [INFO]  Data extraction completed
2025-05-11 16:29:48,603 [INFO]  Starting data transformation
2025-05-11 16:29:49,016 [ERROR] ETL process failed: 'DataFrame' object has no attribute 'aircraft_sk'
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 108, in transform_fact_flight
    aircraft_dim_df.aircraft_sk
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3129, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'aircraft_sk'
2025-05-11 16:29:49,030 [INFO] Closing down clientserver connection
2025-05-11 16:58:16,635 [INFO]  Starting data extraction
2025-05-11 16:58:22,477 [INFO]  Data extraction completed
2025-05-11 16:58:22,477 [INFO]  Starting data transformation
2025-05-11 16:58:23,069 [ERROR] ETL process failed: 'DataFrame' object has no attribute 'aircraft_sk'
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 108, in transform_fact_flight
    aircraft_dim_df.aircraft_sk
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3129, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'aircraft_sk'
2025-05-11 16:58:23,084 [INFO] Closing down clientserver connection
2025-05-11 16:58:32,237 [INFO]  Starting data extraction
2025-05-11 16:58:38,266 [INFO]  Data extraction completed
2025-05-11 16:58:38,266 [INFO]  Starting data transformation
2025-05-11 16:58:38,731 [ERROR] ETL process failed: 'DataFrame' object has no attribute 'aircraft_sk'
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 108, in transform_fact_flight
    aircraft_dim_df.aircraft_sk
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3129, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'aircraft_sk'
2025-05-11 16:58:38,745 [INFO] Closing down clientserver connection
2025-05-11 17:00:16,920 [INFO]  Starting data extraction
2025-05-11 17:00:22,125 [INFO]  Data extraction completed
2025-05-11 17:00:22,126 [INFO]  Starting data transformation
2025-05-11 17:00:22,612 [ERROR] ETL process failed: 'DataFrame' object has no attribute 'route_sk'
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 119, in transform_fact_flight
    route_dim_df.route_sk
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3129, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'route_sk'
2025-05-11 17:00:22,624 [INFO] Closing down clientserver connection
2025-05-11 17:02:36,273 [INFO]  Starting data extraction
2025-05-11 17:02:42,166 [INFO]  Data extraction completed
2025-05-11 17:02:42,167 [INFO]  Starting data transformation
2025-05-11 17:02:42,648 [ERROR] ETL process failed: 'DataFrame' object has no attribute 'dep_delay'
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 125, in transform_fact_flight
    fact_flight.dep_delay == dep_delay_dim_df.dep_delay,
    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3129, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'dep_delay'
2025-05-11 17:02:42,660 [INFO] Closing down clientserver connection
2025-05-18 12:46:27,964 [INFO]  Starting data extraction
2025-05-18 12:46:33,658 [INFO]  Data extraction completed
2025-05-18 12:46:33,658 [INFO]  Starting data transformation
2025-05-18 12:46:35,885 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `aircraft_fk` cannot be resolved. Did you mean one of the following? [`air_time`, `arr_time`, `carrier`, `day`, `distance`].;
'Project [id#164 AS flight_source_id#486, air_time#182, flight_num#178, 'aircraft_fk, 'route_fk, 'dep_delay_fk, 'arr_delay_fk, year#165, month#166, day#167, day_of_week#168, dep_time#169, arr_time#173, sched_dep_time#170, sched_arr_time#174, hour#184, minute#185]
+- Relation [id#164,year#165,month#166,day#167,day_of_week#168,dep_time#169,sched_dep_time#170,dep_delay_time#171,reason_dep_delay#172,arr_time#173,sched_arr_time#174,arr_delay_time#175,reason_arr_delay#176,carrier#177,flight_num#178,tailnum#179,origin#180,destination#181,air_time#182,distance#183,hour#184,minute#185,airline_name#186,departure_city#187,... 5 more fields] csv
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 54, in transform_fact_flight
    fact_flight = flight_df.select(
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `aircraft_fk` cannot be resolved. Did you mean one of the following? [`air_time`, `arr_time`, `carrier`, `day`, `distance`].;
'Project [id#164 AS flight_source_id#486, air_time#182, flight_num#178, 'aircraft_fk, 'route_fk, 'dep_delay_fk, 'arr_delay_fk, year#165, month#166, day#167, day_of_week#168, dep_time#169, arr_time#173, sched_dep_time#170, sched_arr_time#174, hour#184, minute#185]
+- Relation [id#164,year#165,month#166,day#167,day_of_week#168,dep_time#169,sched_dep_time#170,dep_delay_time#171,reason_dep_delay#172,arr_time#173,sched_arr_time#174,arr_delay_time#175,reason_arr_delay#176,carrier#177,flight_num#178,tailnum#179,origin#180,destination#181,air_time#182,distance#183,hour#184,minute#185,airline_name#186,departure_city#187,... 5 more fields] csv

2025-05-18 12:46:35,929 [INFO] Closing down clientserver connection
2025-05-18 13:44:34,831 [INFO]  Starting data extraction
2025-05-18 13:44:40,059 [INFO]  Data extraction completed
2025-05-18 13:44:40,059 [INFO]  Starting data transformation
2025-05-18 13:44:42,217 [ERROR] ETL process failed: 'DataFrame' object has no attribute 'dep_delay_time'
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 68, in transform_fact_flight
    fact_flight.dep_delay_time == dim_dep_delay_df.dep_delay_time,
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3129, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'dep_delay_time'
2025-05-18 13:44:42,237 [INFO] Closing down clientserver connection
2025-05-18 13:48:00,407 [INFO]  Starting data extraction
2025-05-18 13:48:05,275 [INFO]  Data extraction completed
2025-05-18 13:48:05,275 [INFO]  Starting data transformation
2025-05-18 13:48:07,526 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `dep_delay` cannot be resolved. Did you mean one of the following? [`dep_time`, `day`, `dep_delay_time`, `id`, `year`].;
'Project [id#164 AS flight_source_id#469, carrier#177, tailnum#179, origin#180, destination#181, 'dep_delay AS dep_delay_tk#470, 'arr_delay AS arr_delay_tk#471, year#165, month#166, day#167, day_of_week#168, dep_time#169, arr_time#173, sched_dep_time#170, sched_arr_time#174, hour#184, minute#185, air_time#182, flight_num#178, distance#183]
+- Relation [id#164,year#165,month#166,day#167,day_of_week#168,dep_time#169,sched_dep_time#170,dep_delay_time#171,reason_dep_delay#172,arr_time#173,sched_arr_time#174,arr_delay_time#175,reason_arr_delay#176,carrier#177,flight_num#178,tailnum#179,origin#180,destination#181,air_time#182,distance#183,hour#184,minute#185,airline_name#186,departure_city#187,... 5 more fields] csv
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 18, in transform_fact_flight
    fact_flight = flight_df.select(
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `dep_delay` cannot be resolved. Did you mean one of the following? [`dep_time`, `day`, `dep_delay_time`, `id`, `year`].;
'Project [id#164 AS flight_source_id#469, carrier#177, tailnum#179, origin#180, destination#181, 'dep_delay AS dep_delay_tk#470, 'arr_delay AS arr_delay_tk#471, year#165, month#166, day#167, day_of_week#168, dep_time#169, arr_time#173, sched_dep_time#170, sched_arr_time#174, hour#184, minute#185, air_time#182, flight_num#178, distance#183]
+- Relation [id#164,year#165,month#166,day#167,day_of_week#168,dep_time#169,sched_dep_time#170,dep_delay_time#171,reason_dep_delay#172,arr_time#173,sched_arr_time#174,arr_delay_time#175,reason_arr_delay#176,carrier#177,flight_num#178,tailnum#179,origin#180,destination#181,air_time#182,distance#183,hour#184,minute#185,airline_name#186,departure_city#187,... 5 more fields] csv

2025-05-18 13:48:07,548 [INFO] Closing down clientserver connection
2025-05-18 13:49:41,731 [INFO]  Starting data extraction
2025-05-18 13:49:46,714 [INFO]  Data extraction completed
2025-05-18 13:49:46,714 [INFO]  Starting data transformation
2025-05-18 13:49:48,991 [ERROR] ETL process failed: [AMBIGUOUS_REFERENCE] Reference `distance` is ambiguous, could be: [`distance`, `distance`].
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 101, in transform_fact_flight
    final_fact_flight = fact_flight.select(
                        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `distance` is ambiguous, could be: [`distance`, `distance`].
2025-05-18 13:49:49,010 [INFO] Closing down clientserver connection
2025-05-18 22:05:31,507 [INFO]  Starting data extraction
2025-05-18 22:05:37,224 [INFO]  Data extraction completed
2025-05-18 22:05:37,224 [INFO]  Starting data transformation
2025-05-18 22:05:39,692 [ERROR] ETL process failed: [AMBIGUOUS_REFERENCE] Reference `distance` is ambiguous, could be: [`distance`, `distance`].
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 101, in transform_fact_flight
    final_fact_flight = fact_flight.select(
                        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `distance` is ambiguous, could be: [`distance`, `distance`].
2025-05-18 22:05:39,736 [INFO] Closing down clientserver connection
2025-05-18 22:06:55,929 [INFO]  Starting data extraction
2025-05-18 22:07:01,333 [INFO]  Data extraction completed
2025-05-18 22:07:01,333 [INFO]  Starting data transformation
2025-05-18 22:07:03,818 [ERROR] ETL process failed: [AMBIGUOUS_REFERENCE] Reference `distance` is ambiguous, could be: [`distance`, `distance`].
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 101, in transform_fact_flight
    final_fact_flight = fact_flight.select(
                        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `distance` is ambiguous, could be: [`distance`, `distance`].
2025-05-18 22:07:03,841 [INFO] Closing down clientserver connection
2025-05-19 10:54:08,473 [INFO]  Starting data extraction
2025-05-19 10:54:14,562 [INFO]  Data extraction completed
2025-05-19 10:54:14,563 [INFO]  Starting data transformation
2025-05-19 10:54:17,140 [ERROR] ETL process failed: [AMBIGUOUS_REFERENCE] Reference `distance` is ambiguous, could be: [`distance`, `distance`].
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 101, in transform_fact_flight
    final_fact_flight = fact_flight.select(
                        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `distance` is ambiguous, could be: [`distance`, `distance`].
2025-05-19 10:54:17,194 [INFO] Closing down clientserver connection
2025-05-19 11:08:40,315 [INFO]  Starting data extraction
2025-05-19 11:08:45,832 [INFO]  Data extraction completed
2025-05-19 11:08:45,833 [INFO]  Starting data transformation
2025-05-19 11:08:48,221 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `airline_sk` cannot be resolved. Did you mean one of the following? [`airline_fk`, `airline_tk`, `airline_name`, `air_time`, `flight_sk`].;
'Project [flight_sk#1149L, 'airline_sk, 'aircraft_sk, 'route_sk, 'dep_delay_sk, 'arr_delay_sk, 'date_sk, 'time_sk, air_time#182, flight_distance#472, flight_num#178]
+- Project [flight_source_id#469, carrier#177, tailnum#179, origin#180, destination#181, dep_delay_tk#470, arr_delay_tk#471, year#165, month#166, day#167, day_of_week#168, dep_time#169, arr_time#173, sched_dep_time#170, sched_arr_time#174, hour#184, minute#185, air_time#182, flight_num#178, flight_distance#472, airline_tk#260L, carrier#1, airline_name#2, source_id#256, ... 36 more fields]
   +- Join LeftOuter, ((dep_time#169 = dep_time#1006) AND (sched_dep_time#170 = sched_dep_time#1007))
      :- Join LeftOuter, ((((year#165 = year#868) AND (month#166 = month#869)) AND (day#167 = day#870)) AND (day_of_week#168 = day_of_week#871))
      :  :- Join LeftOuter, (arr_delay_tk#471 = cast(arr_delay_tk#380L as double))
      :  :  :- Join LeftOuter, (dep_delay_tk#470 = cast(dep_delay_tk#356L as double))
      :  :  :  :- Join LeftOuter, ((trim(origin#180, None) = trim(origin#29, None)) AND (trim(destination#181, None) = trim(destination#30, None)))
      :  :  :  :  :- Join LeftOuter, (trim(tailnum#179, None) = trim(tailnum#15, None))
      :  :  :  :  :  :- Join LeftOuter, (trim(carrier#177, None) = trim(carrier#1, None))
      :  :  :  :  :  :  :- Project [id#164 AS flight_source_id#469, carrier#177, tailnum#179, origin#180, destination#181, dep_delay_time#171 AS dep_delay_tk#470, arr_delay_time#175 AS arr_delay_tk#471, year#165, month#166, day#167, day_of_week#168, dep_time#169, arr_time#173, sched_dep_time#170, sched_arr_time#174, hour#184, minute#185, air_time#182, flight_num#178, distance#183 AS flight_distance#472]
      :  :  :  :  :  :  :  +- Relation [id#164,year#165,month#166,day#167,day_of_week#168,dep_time#169,sched_dep_time#170,dep_delay_time#171,reason_dep_delay#172,arr_time#173,sched_arr_time#174,arr_delay_time#175,reason_arr_delay#176,carrier#177,flight_num#178,tailnum#179,origin#180,destination#181,air_time#182,distance#183,hour#184,minute#185,airline_name#186,departure_city#187,... 5 more fields] csv
      :  :  :  :  :  :  +- Project [airline_tk#260L, carrier#1, airline_name#2, source_id#256]
      :  :  :  :  :  :     +- Project [source_id#256, carrier#1, airline_name#2, (monotonically_increasing_id() + cast(1 as bigint)) AS airline_tk#260L]
      :  :  :  :  :  :        +- Project [id#0 AS source_id#256, carrier#1, airline_name#2]
      :  :  :  :  :  :           +- Relation [id#0,carrier#1,airline_name#2] JDBCRelation(airline) [numPartitions=1]
      :  :  :  :  :  +- Project [aircraft_tk#282L, tailnum#15, source_id#278, airline_fk#16]
      :  :  :  :  :     +- Project [source_id#278, tailnum#15, airline_fk#16, (monotonically_increasing_id() + cast(1 as bigint)) AS aircraft_tk#282L]
      :  :  :  :  :        +- Project [id#14 AS source_id#278, tailnum#15, airline_fk#16]
      :  :  :  :  :           +- Relation [id#14,tailnum#15,airline_fk#16] JDBCRelation(aircraft) [numPartitions=1]
      :  :  :  :  +- Project [route_tk#311L, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37 AS route_distance#597, source_id#300]
      :  :  :  :     +- Project [route_tk#311L, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37, source_id#300]
      :  :  :  :        +- Project [source_id#300, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37, (monotonically_increasing_id() + cast(1 as bigint)) AS route_tk#311L]
      :  :  :  :           +- Project [id#28 AS source_id#300, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37]
      :  :  :  :              +- Relation [id#28,origin#29,destination#30,departure_city#31,departure_country#32,departure_airport_name#33,destination_city#34,destination_country#35,destination_airport_name#36,distance#37] JDBCRelation(route) [numPartitions=1]
      :  :  :  +- Project [dep_delay_tk#356L, reason#351, delay_time#352, source_id#350]
      :  :  :     +- Project [source_id#350, reason#351, delay_time#352, (monotonically_increasing_id() + cast(1 as bigint)) AS dep_delay_tk#356L]
      :  :  :        +- Project [id#63 AS source_id#350, reason_dep_delay#64 AS reason#351, dep_delay_time#65 AS delay_time#352]
      :  :  :           +- Relation [id#63,reason_dep_delay#64,dep_delay_time#65] JDBCRelation(dep_delay) [numPartitions=1]
      :  :  +- Project [arr_delay_tk#380L, reason#375, delay_time#376, source_id#374]
      :  :     +- Project [source_id#374, reason#375, delay_time#376, (monotonically_increasing_id() + cast(1 as bigint)) AS arr_delay_tk#380L]
      :  :        +- Project [id#77 AS source_id#374, reason_arr_delay#78 AS reason#375, arr_delay_time#79 AS delay_time#376]
      :  :           +- Relation [id#77,reason_arr_delay#78,arr_delay_time#79] JDBCRelation(arr_delay) [numPartitions=1]
      :  +- Project [date_tk#404, year#868, month#869, day#870, day_of_week#871]
      :     +- Project [year#868, month#869, day#870, day_of_week#871, date_tk#404]
      :        +- Project [year#868, month#869, day#870, day_of_week#871, date_tk#404, date_tk#404]
      :           +- Window [row_number() windowspecdefinition(year#868 ASC NULLS FIRST, month#869 ASC NULLS FIRST, day#870 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS date_tk#404], [year#868 ASC NULLS FIRST, month#869 ASC NULLS FIRST, day#870 ASC NULLS FIRST]
      :              +- Project [year#868, month#869, day#870, day_of_week#871]
      :                 +- Deduplicate [year#868, month#869, day#870, day_of_week#871]
      :                    +- Project [year#868, month#869, day#870, day_of_week#871]
      :                       +- Relation [id#867,year#868,month#869,day#870,day_of_week#871,dep_time#872,sched_dep_time#873,dep_delay_time#874,reason_dep_delay#875,arr_time#876,sched_arr_time#877,arr_delay_time#878,reason_arr_delay#879,carrier#880,flight_num#881,tailnum#882,origin#883,destination#884,air_time#885,distance#886,hour#887,minute#888,airline_name#889,departure_city#890,... 5 more fields] csv
      +- Project [time_tk#436, dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022]
         +- Project [dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022, time_tk#436]
            +- Project [dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022, time_tk#436, time_tk#436]
               +- Window [row_number() windowspecdefinition(hour#1021 ASC NULLS FIRST, minute#1022 ASC NULLS FIRST, sched_dep_time#1007 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS time_tk#436], [hour#1021 ASC NULLS FIRST, minute#1022 ASC NULLS FIRST, sched_dep_time#1007 ASC NULLS FIRST]
                  +- Project [dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022]
                     +- Deduplicate [sched_arr_time#1011, minute#1022, dep_time#1006, arr_time#1010, hour#1021, sched_dep_time#1007]
                        +- Project [dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022]
                           +- Relation [id#1001,year#1002,month#1003,day#1004,day_of_week#1005,dep_time#1006,sched_dep_time#1007,dep_delay_time#1008,reason_dep_delay#1009,arr_time#1010,sched_arr_time#1011,arr_delay_time#1012,reason_arr_delay#1013,carrier#1014,flight_num#1015,tailnum#1016,origin#1017,destination#1018,air_time#1019,distance#1020,hour#1021,minute#1022,airline_name#1023,departure_city#1024,... 5 more fields] csv
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\main.py", line 57, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\pipeline.py", line 41, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\transform\facts\fact_flight.py", line 102, in transform_fact_flight
    final_fact_flight = fact_flight.select(
                        ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `airline_sk` cannot be resolved. Did you mean one of the following? [`airline_fk`, `airline_tk`, `airline_name`, `air_time`, `flight_sk`].;
'Project [flight_sk#1149L, 'airline_sk, 'aircraft_sk, 'route_sk, 'dep_delay_sk, 'arr_delay_sk, 'date_sk, 'time_sk, air_time#182, flight_distance#472, flight_num#178]
+- Project [flight_source_id#469, carrier#177, tailnum#179, origin#180, destination#181, dep_delay_tk#470, arr_delay_tk#471, year#165, month#166, day#167, day_of_week#168, dep_time#169, arr_time#173, sched_dep_time#170, sched_arr_time#174, hour#184, minute#185, air_time#182, flight_num#178, flight_distance#472, airline_tk#260L, carrier#1, airline_name#2, source_id#256, ... 36 more fields]
   +- Join LeftOuter, ((dep_time#169 = dep_time#1006) AND (sched_dep_time#170 = sched_dep_time#1007))
      :- Join LeftOuter, ((((year#165 = year#868) AND (month#166 = month#869)) AND (day#167 = day#870)) AND (day_of_week#168 = day_of_week#871))
      :  :- Join LeftOuter, (arr_delay_tk#471 = cast(arr_delay_tk#380L as double))
      :  :  :- Join LeftOuter, (dep_delay_tk#470 = cast(dep_delay_tk#356L as double))
      :  :  :  :- Join LeftOuter, ((trim(origin#180, None) = trim(origin#29, None)) AND (trim(destination#181, None) = trim(destination#30, None)))
      :  :  :  :  :- Join LeftOuter, (trim(tailnum#179, None) = trim(tailnum#15, None))
      :  :  :  :  :  :- Join LeftOuter, (trim(carrier#177, None) = trim(carrier#1, None))
      :  :  :  :  :  :  :- Project [id#164 AS flight_source_id#469, carrier#177, tailnum#179, origin#180, destination#181, dep_delay_time#171 AS dep_delay_tk#470, arr_delay_time#175 AS arr_delay_tk#471, year#165, month#166, day#167, day_of_week#168, dep_time#169, arr_time#173, sched_dep_time#170, sched_arr_time#174, hour#184, minute#185, air_time#182, flight_num#178, distance#183 AS flight_distance#472]
      :  :  :  :  :  :  :  +- Relation [id#164,year#165,month#166,day#167,day_of_week#168,dep_time#169,sched_dep_time#170,dep_delay_time#171,reason_dep_delay#172,arr_time#173,sched_arr_time#174,arr_delay_time#175,reason_arr_delay#176,carrier#177,flight_num#178,tailnum#179,origin#180,destination#181,air_time#182,distance#183,hour#184,minute#185,airline_name#186,departure_city#187,... 5 more fields] csv
      :  :  :  :  :  :  +- Project [airline_tk#260L, carrier#1, airline_name#2, source_id#256]
      :  :  :  :  :  :     +- Project [source_id#256, carrier#1, airline_name#2, (monotonically_increasing_id() + cast(1 as bigint)) AS airline_tk#260L]
      :  :  :  :  :  :        +- Project [id#0 AS source_id#256, carrier#1, airline_name#2]
      :  :  :  :  :  :           +- Relation [id#0,carrier#1,airline_name#2] JDBCRelation(airline) [numPartitions=1]
      :  :  :  :  :  +- Project [aircraft_tk#282L, tailnum#15, source_id#278, airline_fk#16]
      :  :  :  :  :     +- Project [source_id#278, tailnum#15, airline_fk#16, (monotonically_increasing_id() + cast(1 as bigint)) AS aircraft_tk#282L]
      :  :  :  :  :        +- Project [id#14 AS source_id#278, tailnum#15, airline_fk#16]
      :  :  :  :  :           +- Relation [id#14,tailnum#15,airline_fk#16] JDBCRelation(aircraft) [numPartitions=1]
      :  :  :  :  +- Project [route_tk#311L, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37 AS route_distance#597, source_id#300]
      :  :  :  :     +- Project [route_tk#311L, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37, source_id#300]
      :  :  :  :        +- Project [source_id#300, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37, (monotonically_increasing_id() + cast(1 as bigint)) AS route_tk#311L]
      :  :  :  :           +- Project [id#28 AS source_id#300, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37]
      :  :  :  :              +- Relation [id#28,origin#29,destination#30,departure_city#31,departure_country#32,departure_airport_name#33,destination_city#34,destination_country#35,destination_airport_name#36,distance#37] JDBCRelation(route) [numPartitions=1]
      :  :  :  +- Project [dep_delay_tk#356L, reason#351, delay_time#352, source_id#350]
      :  :  :     +- Project [source_id#350, reason#351, delay_time#352, (monotonically_increasing_id() + cast(1 as bigint)) AS dep_delay_tk#356L]
      :  :  :        +- Project [id#63 AS source_id#350, reason_dep_delay#64 AS reason#351, dep_delay_time#65 AS delay_time#352]
      :  :  :           +- Relation [id#63,reason_dep_delay#64,dep_delay_time#65] JDBCRelation(dep_delay) [numPartitions=1]
      :  :  +- Project [arr_delay_tk#380L, reason#375, delay_time#376, source_id#374]
      :  :     +- Project [source_id#374, reason#375, delay_time#376, (monotonically_increasing_id() + cast(1 as bigint)) AS arr_delay_tk#380L]
      :  :        +- Project [id#77 AS source_id#374, reason_arr_delay#78 AS reason#375, arr_delay_time#79 AS delay_time#376]
      :  :           +- Relation [id#77,reason_arr_delay#78,arr_delay_time#79] JDBCRelation(arr_delay) [numPartitions=1]
      :  +- Project [date_tk#404, year#868, month#869, day#870, day_of_week#871]
      :     +- Project [year#868, month#869, day#870, day_of_week#871, date_tk#404]
      :        +- Project [year#868, month#869, day#870, day_of_week#871, date_tk#404, date_tk#404]
      :           +- Window [row_number() windowspecdefinition(year#868 ASC NULLS FIRST, month#869 ASC NULLS FIRST, day#870 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS date_tk#404], [year#868 ASC NULLS FIRST, month#869 ASC NULLS FIRST, day#870 ASC NULLS FIRST]
      :              +- Project [year#868, month#869, day#870, day_of_week#871]
      :                 +- Deduplicate [year#868, month#869, day#870, day_of_week#871]
      :                    +- Project [year#868, month#869, day#870, day_of_week#871]
      :                       +- Relation [id#867,year#868,month#869,day#870,day_of_week#871,dep_time#872,sched_dep_time#873,dep_delay_time#874,reason_dep_delay#875,arr_time#876,sched_arr_time#877,arr_delay_time#878,reason_arr_delay#879,carrier#880,flight_num#881,tailnum#882,origin#883,destination#884,air_time#885,distance#886,hour#887,minute#888,airline_name#889,departure_city#890,... 5 more fields] csv
      +- Project [time_tk#436, dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022]
         +- Project [dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022, time_tk#436]
            +- Project [dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022, time_tk#436, time_tk#436]
               +- Window [row_number() windowspecdefinition(hour#1021 ASC NULLS FIRST, minute#1022 ASC NULLS FIRST, sched_dep_time#1007 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS time_tk#436], [hour#1021 ASC NULLS FIRST, minute#1022 ASC NULLS FIRST, sched_dep_time#1007 ASC NULLS FIRST]
                  +- Project [dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022]
                     +- Deduplicate [sched_arr_time#1011, minute#1022, dep_time#1006, arr_time#1010, hour#1021, sched_dep_time#1007]
                        +- Project [dep_time#1006, arr_time#1010, sched_dep_time#1007, sched_arr_time#1011, hour#1021, minute#1022]
                           +- Relation [id#1001,year#1002,month#1003,day#1004,day_of_week#1005,dep_time#1006,sched_dep_time#1007,dep_delay_time#1008,reason_dep_delay#1009,arr_time#1010,sched_arr_time#1011,arr_delay_time#1012,reason_arr_delay#1013,carrier#1014,flight_num#1015,tailnum#1016,origin#1017,destination#1018,air_time#1019,distance#1020,hour#1021,minute#1022,airline_name#1023,departure_city#1024,... 5 more fields] csv

2025-05-19 11:08:48,244 [INFO] Closing down clientserver connection
2025-05-19 11:15:59,156 [ERROR] ETL process failed: MySQL connector JAR not found at: D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 15, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:18:46,869 [ERROR] ETL process failed: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 15, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:20:09,963 [INFO]  Starting data extraction
2025-05-19 11:20:13,019 [ERROR] CSV file not found: D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint2\flights_najbolji_PROCESSED_20.csv
2025-05-19 11:20:13,019 [ERROR] ETL process failed: CSV file not found: D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint2\flights_najbolji_PROCESSED_20.csv
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 38, in main
    raise FileNotFoundError(f"CSV file not found: {csv_path}")
FileNotFoundError: CSV file not found: D:\fax_\treca_godina\skladistenje_rudarenje\checkpoint2\flights_najbolji_PROCESSED_20.csv
2025-05-19 11:20:13,030 [INFO] Closing down clientserver connection
2025-05-19 11:21:38,749 [ERROR] ETL process failed: MySQL connector JAR not found at: D:ax_	reca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 15, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: D:ax_	reca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:21:43,180 [ERROR] ETL process failed: MySQL connector JAR not found at: D:ax_	reca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 15, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: D:ax_	reca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:22:50,724 [ERROR] ETL process failed: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 15, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:23:05,122 [ERROR] ETL process failed: MySQL connector JAR not found at: \connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 15, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: \connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:25:18,507 [ERROR] ETL process failed: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 15, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:29:32,058 [ERROR] ETL process failed: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 16, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:31:09,322 [ERROR] ETL process failed: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 16, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:31:27,700 [ERROR] ETL process failed: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 26, in main
    spark = get_spark_session()
            ^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 16, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: ..\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 11:33:18,859 [INFO]  Starting data extraction
2025-05-19 11:33:22,413 [ERROR] CSV file not found: D:ax_	reca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint2lights_najbolji_PROCESSED_20.csv
2025-05-19 11:33:22,413 [ERROR] ETL process failed: CSV file not found: D:ax_	reca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint2lights_najbolji_PROCESSED_20.csv
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 38, in main
    raise FileNotFoundError(f"CSV file not found: {csv_path}")
FileNotFoundError: CSV file not found: D:ax_	reca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint2lights_najbolji_PROCESSED_20.csv
2025-05-19 11:33:22,426 [INFO] Closing down clientserver connection
2025-05-19 21:21:25,260 [ERROR] ETL process failed: MySQL connector JAR not found at: D:\fax\treca_godina\skladistenje_rudarenje\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 29, in main
    spark = get_spark_session("Flight_ETL_Process")
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 26, in get_spark_session
    raise FileNotFoundError(f"MySQL connector JAR not found at: {connector_path}")
FileNotFoundError: MySQL connector JAR not found at: D:\fax\treca_godina\skladistenje_rudarenje\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 22:23:28,587 [ERROR] MySQL connector JAR not found at: D:\fax\treca_godina\skladistenje_rudarenje\checkpoint4\connectors\mysql-connector-j-9.2.0.jar
2025-05-19 22:23:28,587 [ERROR] Continuing without Spark session...
2025-05-19 22:25:48,332 [ERROR] ETL process failed: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$14(Executor.scala:1163)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$14$adapted(Executor.scala:1155)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:193)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)
	at org.apache.spark.executor.Executor.updateDependencies(Executor.scala:1155)
	at org.apache.spark.executor.Executor.<init>(Executor.scala:330)
	at org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)
	at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:599)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)
	at org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)
	at org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)
	... 25 more
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 29, in main
    spark = get_spark_session("Flight_ETL_Process")
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 46, in get_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 203, in __init__
    self._do_init(
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 296, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 421, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1587, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$14(Executor.scala:1163)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$14$adapted(Executor.scala:1155)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:193)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)
	at org.apache.spark.executor.Executor.updateDependencies(Executor.scala:1155)
	at org.apache.spark.executor.Executor.<init>(Executor.scala:330)
	at org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)
	at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:599)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)
	at org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)
	at org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)
	... 25 more

2025-05-19 22:25:48,378 [INFO] Closing down clientserver connection
2025-05-19 22:28:42,560 [INFO] Created Spark session
2025-05-19 22:28:42,655 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 22:28:42,655 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 22:28:43,299 [INFO] Created Spark session
2025-05-19 22:28:43,400 [ERROR] ETL process failed: An error occurred while calling o35.jdbc.
: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 39, in main
    mysql_data = extract_all_tables()
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\extract\extract_mysql.py", line 49, in extract_all_tables
    "airline": extract_table("airline"),
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\extract\extract_mysql.py", line 28, in extract_table
    df = spark.read.jdbc(
         ^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\readwriter.py", line 946, in jdbc
    return self._df(self._jreader.jdbc(url, table, jprop))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o35.jdbc.
: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)

2025-05-19 22:28:43,951 [INFO] Closing down clientserver connection
2025-05-19 22:40:45,748 [INFO] Created Spark session
2025-05-19 22:40:45,851 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 22:40:45,851 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 22:40:46,356 [INFO] Created Spark session
2025-05-19 22:40:46,442 [ERROR] ETL process failed: An error occurred while calling o35.jdbc.
: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 39, in main
    mysql_data = extract_all_tables()
                 ^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\extract\extract_mysql.py", line 49, in extract_all_tables
    "airline": extract_table("airline"),
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\extract\extract_mysql.py", line 28, in extract_table
    df = spark.read.jdbc(
         ^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\readwriter.py", line 946, in jdbc
    return self._df(self._jreader.jdbc(url, table, jprop))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o35.jdbc.
: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)

2025-05-19 22:40:46,657 [INFO] Closing down clientserver connection
2025-05-19 22:42:20,076 [ERROR] ETL process failed: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 29, in main
    spark = get_spark_session("Flight_ETL_Process")
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\spark_session.py", line 9, in get_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\java_gateway.py", line 107, in launch_gateway
    raise PySparkRuntimeError(
pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
2025-05-19 22:45:01,186 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 22:45:01,187 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 22:45:04,768 [INFO] Extracting data from CSV file (20% of data)
2025-05-19 22:45:04,768 [ERROR] CSV file not found: D:\fax\treca_godina\skladistenje_rudarenje\checkpoint2\flights_najbolji_PROCESSED_20.csv
2025-05-19 22:45:04,769 [ERROR] CSV file not found: D:\fax\treca_godina\skladistenje_rudarenje\checkpoint2\flights_najbolji_PROCESSED_20.csv
2025-05-19 22:45:04,769 [ERROR] Continuing without Spark session...
2025-05-19 22:45:04,780 [INFO] Closing down clientserver connection
2025-05-19 22:50:03,806 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 22:50:03,807 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 22:50:07,713 [INFO] Extracting data from CSV file (20% of data)
2025-05-19 22:50:10,656 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-19 22:50:10,658 [INFO] Transforming MySQL data (80%)
2025-05-19 22:50:15,102 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `carrier` cannot be resolved. Did you mean one of the following? [`id`, `year`, `month`, `day`, `day_of_week`, `dep_time`, `arr_time`, `sched_dep_time`, `sched_arr_time`, `hour`, `minute`, `air_time`, `flight_num`, `dep_delay_fk`, `arr_delay_fk`, `route_fk`, `aircraft_fk`, `source_system`].
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 62, in main
    transformed_mysql = run_transformations(mysql_data)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\pipeline.py", line 50, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\facts\fact_flight.py", line 37, in transform_fact_flight
    flight_df["carrier"] == dim_airline_df["carrier"],
    ~~~~~~~~~^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3080, in __getitem__
    jc = self._jdf.apply(item)
         ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `carrier` cannot be resolved. Did you mean one of the following? [`id`, `year`, `month`, `day`, `day_of_week`, `dep_time`, `arr_time`, `sched_dep_time`, `sched_arr_time`, `hour`, `minute`, `air_time`, `flight_num`, `dep_delay_fk`, `arr_delay_fk`, `route_fk`, `aircraft_fk`, `source_system`].
2025-05-19 22:50:15,375 [INFO] Closing down clientserver connection
2025-05-19 22:53:52,157 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 22:53:52,157 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 22:53:55,710 [INFO] Extracting data from CSV file (20% of data)
2025-05-19 22:53:58,084 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-19 22:53:58,084 [INFO] Transforming MySQL data (80%)
2025-05-19 22:54:02,338 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`tailnum`, `aircraft_tk`].;
'Project ['id, tailnum#15, 'airline_fk]
+- Project [aircraft_tk#360L, tailnum#15]
   +- Project [tailnum#15, (monotonically_increasing_id() + cast(1 as bigint)) AS aircraft_tk#360L]
      +- Filter atleastnnonnulls(1, tailnum#15)
         +- Deduplicate [tailnum#15]
            +- Project [tailnum#15]
               +- Project [id#14, tailnum#15, airline_fk#16, MYSQL AS source_system#261]
                  +- Relation [id#14,tailnum#15,airline_fk#16] JDBCRelation(aircraft) [numPartitions=1]
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 62, in main
    transformed_mysql = run_transformations(mysql_data)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\pipeline.py", line 50, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\facts\fact_flight.py", line 36, in transform_fact_flight
    dim_aircraft_df.select("id", "tailnum", "airline_fk"),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`tailnum`, `aircraft_tk`].;
'Project ['id, tailnum#15, 'airline_fk]
+- Project [aircraft_tk#360L, tailnum#15]
   +- Project [tailnum#15, (monotonically_increasing_id() + cast(1 as bigint)) AS aircraft_tk#360L]
      +- Filter atleastnnonnulls(1, tailnum#15)
         +- Deduplicate [tailnum#15]
            +- Project [tailnum#15]
               +- Project [id#14, tailnum#15, airline_fk#16, MYSQL AS source_system#261]
                  +- Relation [id#14,tailnum#15,airline_fk#16] JDBCRelation(aircraft) [numPartitions=1]

2025-05-19 22:54:02,746 [INFO] Closing down clientserver connection
2025-05-19 22:56:00,926 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 22:56:00,928 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 22:56:04,615 [INFO] Extracting data from CSV file (20% of data)
2025-05-19 22:56:06,990 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-19 22:56:06,990 [INFO] Transforming MySQL data (80%)
2025-05-19 22:56:11,079 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`tailnum`, `aircraft_tk`].;
'Project [aircraft_tk#360L, tailnum#15, 'airline_id]
+- Project [aircraft_tk#360L, tailnum#15]
   +- Project [tailnum#15, (monotonically_increasing_id() + cast(1 as bigint)) AS aircraft_tk#360L]
      +- Filter atleastnnonnulls(1, tailnum#15)
         +- Deduplicate [tailnum#15]
            +- Project [tailnum#15]
               +- Project [id#14, tailnum#15, airline_fk#16, MYSQL AS source_system#261]
                  +- Relation [id#14,tailnum#15,airline_fk#16] JDBCRelation(aircraft) [numPartitions=1]
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 62, in main
    transformed_mysql = run_transformations(mysql_data)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\pipeline.py", line 50, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\facts\fact_flight.py", line 42, in transform_fact_flight
    dim_aircraft_df.select("aircraft_tk", "tailnum", "airline_id"),  # Changed from id to aircraft_tk and added airline_id
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `airline_id` cannot be resolved. Did you mean one of the following? [`tailnum`, `aircraft_tk`].;
'Project [aircraft_tk#360L, tailnum#15, 'airline_id]
+- Project [aircraft_tk#360L, tailnum#15]
   +- Project [tailnum#15, (monotonically_increasing_id() + cast(1 as bigint)) AS aircraft_tk#360L]
      +- Filter atleastnnonnulls(1, tailnum#15)
         +- Deduplicate [tailnum#15]
            +- Project [tailnum#15]
               +- Project [id#14, tailnum#15, airline_fk#16, MYSQL AS source_system#261]
                  +- Relation [id#14,tailnum#15,airline_fk#16] JDBCRelation(aircraft) [numPartitions=1]

2025-05-19 22:56:11,490 [INFO] Closing down clientserver connection
2025-05-19 23:01:52,851 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 23:01:52,852 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 23:01:56,611 [INFO] Extracting data from CSV file (20% of data)
2025-05-19 23:01:59,330 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-19 23:01:59,330 [INFO] Transforming MySQL data (80%)
2025-05-19 23:02:03,625 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `tailnum` cannot be resolved. Did you mean one of the following? [`id`, `year`, `month`, `day`, `day_of_week`, `dep_time`, `arr_time`, `sched_dep_time`, `sched_arr_time`, `hour`, `minute`, `air_time`, `flight_num`, `dep_delay_fk`, `arr_delay_fk`, `route_fk`, `aircraft_fk`, `source_system`].
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 62, in main
    transformed_mysql = run_transformations(mysql_data)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\pipeline.py", line 50, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\facts\fact_flight.py", line 33, in transform_fact_flight
    flight_df["tailnum"] == dim_aircraft_df["tailnum"],
    ~~~~~~~~~^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 3080, in __getitem__
    jc = self._jdf.apply(item)
         ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `tailnum` cannot be resolved. Did you mean one of the following? [`id`, `year`, `month`, `day`, `day_of_week`, `dep_time`, `arr_time`, `sched_dep_time`, `sched_arr_time`, `hour`, `minute`, `air_time`, `flight_num`, `dep_delay_fk`, `arr_delay_fk`, `route_fk`, `aircraft_fk`, `source_system`].
2025-05-19 23:02:03,949 [INFO] Closing down clientserver connection
2025-05-19 23:03:40,106 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 23:03:40,107 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 23:03:43,605 [INFO] Extracting data from CSV file (20% of data)
2025-05-19 23:03:45,824 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-19 23:03:45,824 [INFO] Transforming MySQL data (80%)
2025-05-19 23:03:54,111 [INFO] Processing combined data (MySQL 80% + CSV 20%)
2025-05-19 23:03:54,192 [INFO] Transforming combined data
2025-05-19 23:04:02,014 [INFO] ======= STARTING LOAD PHASE =======
2025-05-19 23:04:02,173 [INFO] Loading table 'dim_airline' with 16 records
2025-05-19 23:04:02,799 [INFO] Loading table 'dim_aircraft' with 3994 records
2025-05-19 23:04:03,835 [INFO] Loading table 'dim_route' with 223 records
2025-05-19 23:04:04,381 [INFO] Loading table 'dim_dep_delay' with 2647 records
2025-05-19 23:04:05,206 [INFO] Loading table 'dim_arr_delay' with 2690 records
2025-05-19 23:04:06,321 [INFO] Loading table 'dim_date' with 365 records
2025-05-19 23:04:08,236 [INFO] Loading table 'dim_time' with 320438 records
2025-05-19 23:04:15,068 [ERROR] ETL process failed: An error occurred while calling o680.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 261.0 failed 1 times, most recent failure: Lost task 1.0 in stage 261.0 (TID 266) (DESKTOP-AJ23DVH.mshome.net executor driver): java.sql.BatchUpdateException: Data truncation: Data too long for column 'sched_dep_time' at row 1
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:214)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:885)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:466)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:858)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'sched_dep_time' at row 1
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:96)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:990)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1168)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:864)
	... 19 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
	at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.BatchUpdateException: Data truncation: Data too long for column 'sched_dep_time' at row 1
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:214)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:885)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:466)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:858)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more
Caused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'sched_dep_time' at row 1
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:96)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:990)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1168)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:864)
	... 19 more
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 103, in main
    write_spark_df_to_mysql(df, table_name)
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\load\run_loading.py", line 24, in write_spark_df_to_mysql
    spark_df.write.jdbc(
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o680.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 261.0 failed 1 times, most recent failure: Lost task 1.0 in stage 261.0 (TID 266) (DESKTOP-AJ23DVH.mshome.net executor driver): java.sql.BatchUpdateException: Data truncation: Data too long for column 'sched_dep_time' at row 1
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:214)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:885)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:466)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:858)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'sched_dep_time' at row 1
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:96)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:990)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1168)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:864)
	... 19 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
	at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.sql.BatchUpdateException: Data truncation: Data too long for column 'sched_dep_time' at row 1
	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:214)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:885)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:466)
	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:858)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	... 1 more
Caused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'sched_dep_time' at row 1
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:96)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:990)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1168)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:864)
	... 19 more

2025-05-19 23:04:15,546 [INFO] Closing down clientserver connection
2025-05-19 23:06:43,123 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-19 23:06:43,124 [INFO] Extracting data from MySQL database (80% of data)
2025-05-19 23:06:46,327 [INFO] Extracting data from CSV file (20% of data)
2025-05-19 23:06:48,510 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-19 23:06:48,510 [INFO] Transforming MySQL data (80%)
2025-05-19 23:06:56,673 [INFO] Processing combined data (MySQL 80% + CSV 20%)
2025-05-19 23:06:56,713 [INFO] Transforming combined data
2025-05-19 23:07:04,841 [INFO] ======= STARTING LOAD PHASE =======
2025-05-19 23:07:04,969 [INFO] Loading table 'dim_airline' with 16 records
2025-05-19 23:07:05,964 [INFO] Loading table 'dim_aircraft' with 3994 records
2025-05-19 23:07:06,970 [INFO] Loading table 'dim_route' with 223 records
2025-05-19 23:07:07,535 [INFO] Loading table 'dim_dep_delay' with 2647 records
2025-05-19 23:07:08,356 [INFO] Loading table 'dim_arr_delay' with 2690 records
2025-05-19 23:07:09,456 [INFO] Loading table 'dim_date' with 365 records
2025-05-19 23:07:11,436 [INFO] Loading table 'dim_time' with 320438 records
2025-05-19 23:07:21,938 [INFO] Loading table 'fact_flight' with 327346 records
2025-05-19 23:07:32,286 [INFO] ETL process completed successfully
2025-05-19 23:07:32,842 [INFO] Closing down clientserver connection
2025-05-20 00:43:16,837 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-20 00:43:16,837 [INFO] Extracting data from MySQL database (80% of data)
2025-05-20 00:43:20,209 [INFO] Extracting data from CSV file (20% of data)
2025-05-20 00:43:22,430 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-20 00:43:22,430 [INFO] Transforming MySQL data (80%)
2025-05-20 00:43:26,499 [ERROR] ETL process failed: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`tailnum`, `aircraft_tk`].; line 17 pos 20;
'Project [f.*, 'a.tailnum, 'air.carrier, 'air.airline_name, 'r.origin, 'r.destination, 'r.distance, 'dd.reason_dep_delay, 'dd.dep_delay_time, 'ad.reason_arr_delay, 'ad.arr_delay_time]
+- 'Join LeftOuter, ('f.arr_delay_fk = 'ad.id)
   :- 'Join LeftOuter, ('f.dep_delay_fk = 'dd.id)
   :  :- 'Join LeftOuter, ('f.route_fk = 'r.id)
   :  :  :- 'Join LeftOuter, (1 = 1)
   :  :  :  :- 'Join LeftOuter, ('f.aircraft_fk = 'a.id)
   :  :  :  :  :- SubqueryAlias f
   :  :  :  :  :  +- SubqueryAlias flight
   :  :  :  :  :     +- View (`flight`, [id#91,year#92,month#93,day#94,day_of_week#95,dep_time#96,arr_time#97,sched_dep_time#98,sched_arr_time#99,hour#100,minute#101,air_time#102,flight_num#103,dep_delay_fk#104,arr_delay_fk#105,route_fk#106,aircraft_fk#107,source_system#288])
   :  :  :  :  :        +- Project [id#91, year#92, month#93, day#94, day_of_week#95, dep_time#96, arr_time#97, sched_dep_time#98, sched_arr_time#99, hour#100, minute#101, air_time#102, flight_num#103, dep_delay_fk#104, arr_delay_fk#105, route_fk#106, aircraft_fk#107, MYSQL AS source_system#288]
   :  :  :  :  :           +- Relation [id#91,year#92,month#93,day#94,day_of_week#95,dep_time#96,arr_time#97,sched_dep_time#98,sched_arr_time#99,hour#100,minute#101,air_time#102,flight_num#103,dep_delay_fk#104,arr_delay_fk#105,route_fk#106,aircraft_fk#107] JDBCRelation(flight) [numPartitions=1]
   :  :  :  :  +- 'SubqueryAlias a
   :  :  :  :     +- 'Project ['id, tailnum#15]
   :  :  :  :        +- SubqueryAlias dim_aircraft
   :  :  :  :           +- View (`dim_aircraft`, [aircraft_tk#360L,tailnum#15])
   :  :  :  :              +- Project [aircraft_tk#360L, tailnum#15]
   :  :  :  :                 +- Project [tailnum#15, (monotonically_increasing_id() + cast(1 as bigint)) AS aircraft_tk#360L]
   :  :  :  :                    +- Filter atleastnnonnulls(1, tailnum#15)
   :  :  :  :                       +- Deduplicate [tailnum#15]
   :  :  :  :                          +- Project [tailnum#15]
   :  :  :  :                             +- Project [id#14, tailnum#15, airline_fk#16, MYSQL AS source_system#261]
   :  :  :  :                                +- Relation [id#14,tailnum#15,airline_fk#16] JDBCRelation(aircraft) [numPartitions=1]
   :  :  :  +- 'SubqueryAlias air
   :  :  :     +- 'Project ['id, carrier#1, airline_name#2]
   :  :  :        +- SubqueryAlias dim_airline
   :  :  :           +- View (`dim_airline`, [airline_tk#341L,carrier#1,airline_name#2])
   :  :  :              +- Project [airline_tk#341L, carrier#1, airline_name#2]
   :  :  :                 +- Project [carrier#1, airline_name#2, (monotonically_increasing_id() + cast(1 as bigint)) AS airline_tk#341L]
   :  :  :                    +- Deduplicate [carrier#1]
   :  :  :                       +- Project [carrier#1, airline_name#2]
   :  :  :                          +- Project [id#0, carrier#1, airline_name#2, MYSQL AS source_system#256]
   :  :  :                             +- Relation [id#0,carrier#1,airline_name#2] JDBCRelation(airline) [numPartitions=1]
   :  :  +- 'SubqueryAlias r
   :  :     +- 'Project ['id, origin#29, destination#30, distance#37]
   :  :        +- SubqueryAlias dim_route
   :  :           +- View (`dim_route`, [route_tk#381L,origin#29,destination#30,departure_city#31,departure_country#32,departure_airport_name#33,destination_city#34,destination_country#35,destination_airport_name#36,distance#37])
   :  :              +- Project [route_tk#381L, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37]
   :  :                 +- Project [origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37, (monotonically_increasing_id() + cast(1 as bigint)) AS route_tk#381L]
   :  :                    +- Deduplicate [origin#29, destination#30]
   :  :                       +- Project [origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37]
   :  :                          +- Project [id#28, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37, MYSQL AS source_system#266]
   :  :                             +- Relation [id#28,origin#29,destination#30,departure_city#31,departure_country#32,departure_airport_name#33,destination_city#34,destination_country#35,destination_airport_name#36,distance#37] JDBCRelation(route) [numPartitions=1]
   :  +- 'SubqueryAlias dd
   :     +- 'Project ['id, 'reason_dep_delay, 'dep_delay_time]
   :        +- SubqueryAlias dim_dep_delay
   :           +- View (`dim_dep_delay`, [dep_delay_tk#435L,reason#431,delay_time#432])
   :              +- Project [dep_delay_tk#435L, reason#431, delay_time#432]
   :                 +- Project [reason#431, delay_time#432, (monotonically_increasing_id() + cast(1 as bigint)) AS dep_delay_tk#435L]
   :                    +- Deduplicate [reason#431, delay_time#432]
   :                       +- Project [reason_dep_delay#64 AS reason#431, dep_delay_time#65 AS delay_time#432]
   :                          +- Project [id#63, reason_dep_delay#64, dep_delay_time#65, MYSQL AS source_system#278]
   :                             +- Relation [id#63,reason_dep_delay#64,dep_delay_time#65] JDBCRelation(dep_delay) [numPartitions=1]
   +- 'SubqueryAlias ad
      +- 'Project ['id, 'reason_arr_delay, 'arr_delay_time]
         +- SubqueryAlias dim_arr_delay
            +- View (`dim_arr_delay`, [arr_delay_tk#454L,reason#450,delay_time#451])
               +- Project [arr_delay_tk#454L, reason#450, delay_time#451]
                  +- Project [reason#450, delay_time#451, (monotonically_increasing_id() + cast(1 as bigint)) AS arr_delay_tk#454L]
                     +- Deduplicate [reason#450, delay_time#451]
                        +- Project [reason_arr_delay#78 AS reason#450, arr_delay_time#79 AS delay_time#451]
                           +- Project [id#77, reason_arr_delay#78, arr_delay_time#79, MYSQL AS source_system#283]
                              +- Relation [id#77,reason_arr_delay#78,arr_delay_time#79] JDBCRelation(arr_delay) [numPartitions=1]
Traceback (most recent call last):
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\main.py", line 62, in main
    transformed_mysql = run_transformations(mysql_data)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\pipeline.py", line 50, in run_transformations
    fact_flight_df = transform_fact_flight(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\fax_\treca_godina\skladistenje_rudarenje\SRP_Projekt\checkpoint4\transform\facts\fact_flight.py", line 79, in transform_fact_flight
    fact_base = spark.sql(fact_table_sql)
                ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\session.py", line 1631, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\gabri\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`tailnum`, `aircraft_tk`].; line 17 pos 20;
'Project [f.*, 'a.tailnum, 'air.carrier, 'air.airline_name, 'r.origin, 'r.destination, 'r.distance, 'dd.reason_dep_delay, 'dd.dep_delay_time, 'ad.reason_arr_delay, 'ad.arr_delay_time]
+- 'Join LeftOuter, ('f.arr_delay_fk = 'ad.id)
   :- 'Join LeftOuter, ('f.dep_delay_fk = 'dd.id)
   :  :- 'Join LeftOuter, ('f.route_fk = 'r.id)
   :  :  :- 'Join LeftOuter, (1 = 1)
   :  :  :  :- 'Join LeftOuter, ('f.aircraft_fk = 'a.id)
   :  :  :  :  :- SubqueryAlias f
   :  :  :  :  :  +- SubqueryAlias flight
   :  :  :  :  :     +- View (`flight`, [id#91,year#92,month#93,day#94,day_of_week#95,dep_time#96,arr_time#97,sched_dep_time#98,sched_arr_time#99,hour#100,minute#101,air_time#102,flight_num#103,dep_delay_fk#104,arr_delay_fk#105,route_fk#106,aircraft_fk#107,source_system#288])
   :  :  :  :  :        +- Project [id#91, year#92, month#93, day#94, day_of_week#95, dep_time#96, arr_time#97, sched_dep_time#98, sched_arr_time#99, hour#100, minute#101, air_time#102, flight_num#103, dep_delay_fk#104, arr_delay_fk#105, route_fk#106, aircraft_fk#107, MYSQL AS source_system#288]
   :  :  :  :  :           +- Relation [id#91,year#92,month#93,day#94,day_of_week#95,dep_time#96,arr_time#97,sched_dep_time#98,sched_arr_time#99,hour#100,minute#101,air_time#102,flight_num#103,dep_delay_fk#104,arr_delay_fk#105,route_fk#106,aircraft_fk#107] JDBCRelation(flight) [numPartitions=1]
   :  :  :  :  +- 'SubqueryAlias a
   :  :  :  :     +- 'Project ['id, tailnum#15]
   :  :  :  :        +- SubqueryAlias dim_aircraft
   :  :  :  :           +- View (`dim_aircraft`, [aircraft_tk#360L,tailnum#15])
   :  :  :  :              +- Project [aircraft_tk#360L, tailnum#15]
   :  :  :  :                 +- Project [tailnum#15, (monotonically_increasing_id() + cast(1 as bigint)) AS aircraft_tk#360L]
   :  :  :  :                    +- Filter atleastnnonnulls(1, tailnum#15)
   :  :  :  :                       +- Deduplicate [tailnum#15]
   :  :  :  :                          +- Project [tailnum#15]
   :  :  :  :                             +- Project [id#14, tailnum#15, airline_fk#16, MYSQL AS source_system#261]
   :  :  :  :                                +- Relation [id#14,tailnum#15,airline_fk#16] JDBCRelation(aircraft) [numPartitions=1]
   :  :  :  +- 'SubqueryAlias air
   :  :  :     +- 'Project ['id, carrier#1, airline_name#2]
   :  :  :        +- SubqueryAlias dim_airline
   :  :  :           +- View (`dim_airline`, [airline_tk#341L,carrier#1,airline_name#2])
   :  :  :              +- Project [airline_tk#341L, carrier#1, airline_name#2]
   :  :  :                 +- Project [carrier#1, airline_name#2, (monotonically_increasing_id() + cast(1 as bigint)) AS airline_tk#341L]
   :  :  :                    +- Deduplicate [carrier#1]
   :  :  :                       +- Project [carrier#1, airline_name#2]
   :  :  :                          +- Project [id#0, carrier#1, airline_name#2, MYSQL AS source_system#256]
   :  :  :                             +- Relation [id#0,carrier#1,airline_name#2] JDBCRelation(airline) [numPartitions=1]
   :  :  +- 'SubqueryAlias r
   :  :     +- 'Project ['id, origin#29, destination#30, distance#37]
   :  :        +- SubqueryAlias dim_route
   :  :           +- View (`dim_route`, [route_tk#381L,origin#29,destination#30,departure_city#31,departure_country#32,departure_airport_name#33,destination_city#34,destination_country#35,destination_airport_name#36,distance#37])
   :  :              +- Project [route_tk#381L, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37]
   :  :                 +- Project [origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37, (monotonically_increasing_id() + cast(1 as bigint)) AS route_tk#381L]
   :  :                    +- Deduplicate [origin#29, destination#30]
   :  :                       +- Project [origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37]
   :  :                          +- Project [id#28, origin#29, destination#30, departure_city#31, departure_country#32, departure_airport_name#33, destination_city#34, destination_country#35, destination_airport_name#36, distance#37, MYSQL AS source_system#266]
   :  :                             +- Relation [id#28,origin#29,destination#30,departure_city#31,departure_country#32,departure_airport_name#33,destination_city#34,destination_country#35,destination_airport_name#36,distance#37] JDBCRelation(route) [numPartitions=1]
   :  +- 'SubqueryAlias dd
   :     +- 'Project ['id, 'reason_dep_delay, 'dep_delay_time]
   :        +- SubqueryAlias dim_dep_delay
   :           +- View (`dim_dep_delay`, [dep_delay_tk#435L,reason#431,delay_time#432])
   :              +- Project [dep_delay_tk#435L, reason#431, delay_time#432]
   :                 +- Project [reason#431, delay_time#432, (monotonically_increasing_id() + cast(1 as bigint)) AS dep_delay_tk#435L]
   :                    +- Deduplicate [reason#431, delay_time#432]
   :                       +- Project [reason_dep_delay#64 AS reason#431, dep_delay_time#65 AS delay_time#432]
   :                          +- Project [id#63, reason_dep_delay#64, dep_delay_time#65, MYSQL AS source_system#278]
   :                             +- Relation [id#63,reason_dep_delay#64,dep_delay_time#65] JDBCRelation(dep_delay) [numPartitions=1]
   +- 'SubqueryAlias ad
      +- 'Project ['id, 'reason_arr_delay, 'arr_delay_time]
         +- SubqueryAlias dim_arr_delay
            +- View (`dim_arr_delay`, [arr_delay_tk#454L,reason#450,delay_time#451])
               +- Project [arr_delay_tk#454L, reason#450, delay_time#451]
                  +- Project [reason#450, delay_time#451, (monotonically_increasing_id() + cast(1 as bigint)) AS arr_delay_tk#454L]
                     +- Deduplicate [reason#450, delay_time#451]
                        +- Project [reason_arr_delay#78 AS reason#450, arr_delay_time#79 AS delay_time#451]
                           +- Project [id#77, reason_arr_delay#78, arr_delay_time#79, MYSQL AS source_system#283]
                              +- Relation [id#77,reason_arr_delay#78,arr_delay_time#79] JDBCRelation(arr_delay) [numPartitions=1]

2025-05-20 00:43:26,914 [INFO] Closing down clientserver connection
2025-05-20 00:44:17,059 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-20 00:44:17,060 [INFO] Extracting data from MySQL database (80% of data)
2025-05-20 00:44:20,842 [INFO] Extracting data from CSV file (20% of data)
2025-05-20 00:44:23,469 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-20 00:44:23,469 [INFO] Transforming MySQL data (80%)
2025-05-20 00:44:32,402 [INFO] Processing combined data (MySQL 80% + CSV 20%)
2025-05-20 00:44:32,445 [INFO] Transforming combined data
2025-05-20 00:44:40,810 [INFO] ======= STARTING LOAD PHASE =======
2025-05-20 00:44:40,941 [INFO] Loading table 'dim_airline' with 16 records
2025-05-20 00:44:41,541 [INFO] Loading table 'dim_aircraft' with 3994 records
2025-05-20 00:44:42,694 [INFO] Loading table 'dim_route' with 223 records
2025-05-20 00:44:43,344 [INFO] Loading table 'dim_dep_delay' with 2647 records
2025-05-20 00:44:44,403 [INFO] Loading table 'dim_arr_delay' with 2690 records
2025-05-20 00:44:45,731 [INFO] Loading table 'dim_date' with 365 records
2025-05-20 00:44:47,990 [INFO] Loading table 'dim_time' with 320438 records
2025-05-20 00:44:56,766 [INFO] Loading table 'fact_flight' with 327346 records
2025-05-20 00:45:08,441 [INFO] ETL process completed successfully
2025-05-20 00:45:08,626 [INFO] Closing down clientserver connection
2025-05-20 01:03:07,516 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-20 01:03:07,517 [INFO] Extracting data from MySQL database (80% of data)
2025-05-20 01:03:10,965 [INFO] Extracting data from CSV file (20% of data)
2025-05-20 01:03:13,519 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-20 01:03:13,520 [INFO] Transforming MySQL data (80%)
2025-05-20 01:03:21,849 [INFO] Processing combined data (MySQL 80% + CSV 20%)
2025-05-20 01:03:21,889 [INFO] Transforming combined data
2025-05-20 01:03:29,758 [INFO] ======= STARTING LOAD PHASE =======
2025-05-20 01:03:29,946 [INFO] Loading table 'dim_airline' with 16 records
2025-05-20 01:03:30,431 [INFO] Loading table 'dim_aircraft' with 3994 records
2025-05-20 01:03:31,468 [INFO] Loading table 'dim_route' with 223 records
2025-05-20 01:03:32,027 [INFO] Loading table 'dim_dep_delay' with 2647 records
2025-05-20 01:03:32,848 [INFO] Loading table 'dim_arr_delay' with 2690 records
2025-05-20 01:03:33,909 [INFO] Loading table 'dim_date' with 365 records
2025-05-20 01:03:35,882 [INFO] Loading table 'dim_time' with 320438 records
2025-05-20 01:03:43,443 [INFO] Loading table 'fact_flight' with 327346 records
2025-05-20 01:03:54,417 [INFO] ETL process completed successfully
2025-05-20 01:03:54,589 [INFO] Closing down clientserver connection
2025-05-20 01:15:53,159 [INFO] ======= STARTING EXTRACT PHASE =======
2025-05-20 01:15:53,159 [INFO] Extracting data from MySQL database (80% of data)
2025-05-20 01:15:56,375 [INFO] Extracting data from CSV file (20% of data)
2025-05-20 01:15:58,416 [INFO] ======= STARTING TRANSFORM PHASE =======
2025-05-20 01:15:58,416 [INFO] Transforming MySQL data (80%)
2025-05-20 01:16:06,352 [INFO] Processing combined data (MySQL 80% + CSV 20%)
2025-05-20 01:16:06,393 [INFO] Transforming combined data
2025-05-20 01:16:14,099 [INFO] ======= STARTING LOAD PHASE =======
2025-05-20 01:16:14,223 [INFO] Loading table 'dim_airline' with 16 records
2025-05-20 01:16:14,718 [INFO] Loading table 'dim_aircraft' with 3994 records
2025-05-20 01:16:15,686 [INFO] Loading table 'dim_route' with 223 records
2025-05-20 01:16:16,240 [INFO] Loading table 'dim_dep_delay' with 2647 records
2025-05-20 01:16:17,071 [INFO] Loading table 'dim_arr_delay' with 2690 records
2025-05-20 01:16:18,184 [INFO] Loading table 'dim_date' with 365 records
2025-05-20 01:16:20,066 [INFO] Loading table 'dim_time' with 320438 records
2025-05-20 01:16:28,130 [INFO] Loading table 'fact_flight' with 327346 records
2025-05-20 01:16:39,066 [INFO] ETL process completed successfully
2025-05-20 01:16:39,262 [INFO] Closing down clientserver connection
